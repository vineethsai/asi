---
id: "AVD-2024-001"
title: "ChatGPT Plugin Prompt Injection via Indirect Input"
description: "Attackers can inject malicious prompts through external content (documents, websites) that manipulate ChatGPT's behavior when accessed via plugins, leading to data exfiltration and unauthorized actions."
severity: "high"
cvss_score: 7.5
cve_id: null
disclosure_date: "2024-03-15"
discovered_by: "Embrace the Red"
affected_systems:
  - "OpenAI ChatGPT"
  - "ChatGPT Plugins"
  - "GPT-4 with browsing"
vulnerability_type: "prompt_injection"
attack_vector: "indirect_prompt_injection"
impact:
  confidentiality: true
  integrity: true
  availability: false
technical_details:
  attack_method: "Malicious prompts embedded in external content (PDFs, websites, documents) that are processed by ChatGPT plugins"
  exploitation_complexity: "medium"
  user_interaction: "required"
  scope: "changed"
threat_mapping:
  - "t1"
  - "t6"
mitigation_mapping:
  - "m4"
  - "m11"
  - "m1"
proof_of_concept:
  available: true
  description: "Demonstrated data exfiltration via image markdown rendering with embedded conversation data in URLs"
remediation:
  vendor_response: "OpenAI implemented content filtering and URL validation"
  workarounds:
    - "Disable plugin access to external content"
    - "Implement content sanitization"
    - "Use allowlist for trusted domains"
  patch_status: "partially_fixed"
references:
  - title: "ChatGPT Data Exfiltration via Indirect Prompt Injection"
    url: "https://embracethered.com/blog/posts/2023/chatgpt-data-exfiltration-via-indirect-prompt-injection/"
    type: "blog_post"
tags:
  - "prompt_injection"
  - "data_exfiltration"
  - "plugins"
  - "llm"
status: "disclosed"
last_updated: "2024-03-15"
---

# ChatGPT Plugin Prompt Injection via Indirect Input

## Overview

This vulnerability demonstrates how attackers can manipulate ChatGPT's behavior through indirect prompt injection via external content. When ChatGPT plugins access external documents, websites, or files, malicious prompts embedded within that content can override the original user intent and cause the AI to perform unauthorized actions.

## Technical Details

The attack works by embedding malicious instructions within external content that ChatGPT plugins process. These instructions can:

- Override the original user prompt
- Exfiltrate conversation data
- Perform unauthorized actions on behalf of the user
- Manipulate the AI's responses to spread misinformation

## Impact

This vulnerability has significant implications for:
- **Data Privacy**: Sensitive conversation data can be exfiltrated
- **Trust**: Users may receive manipulated responses without knowing
- **Security**: Unauthorized actions can be performed in the user's context

## Mitigation

Organizations using ChatGPT plugins should:
1. Implement strict content filtering for external sources
2. Use allowlists for trusted domains only
3. Sanitize all external content before processing
4. Monitor for unusual AI behavior patterns 